{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# ¡Bienvenidos a la Semana 2!\n",
    "\n",
    "## API de modelos Frontier\n",
    "\n",
    "En la Semana 1, usamos múltiples LLM de Frontier a través de su interfaz de chat y nos conectamos con la API de OpenAI.\n",
    "\n",
    "Hoy nos conectaremos con las API de Anthropic y Google, así como también con OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Nota importante: léame</h2>\n",
    "<span style=\"color:#900;\">Estoy mejorando continuamente estos laboratorios, agregando más ejemplos y ejercicios.\n",
    "Al comienzo de cada semana, vale la pena verificar que tenga el código más reciente.<br/>\n",
    "Primero, haga un <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull y combine los cambios según sea necesario</a>. ¿Tiene algún problema? Intente preguntarle a ChatGPT para que le aclare cómo realizar la fusión, o póngase en contacto conmigo.<br/><br/>\n",
    "Después de haber obtenido el código, desde el directorio llm_engineering, en un indicador de Anaconda (PC) o Terminal (Mac), ejecute:<br/>\n",
    "<code>conda env update --f environment.yml --prune</code><br/>\n",
    "O si utilizó virtualenv en lugar de Anaconda, ejecute esto desde su entorno activado en un Powershell (PC) o Terminal (Mac):<br/>\n",
    "<code>pip install -r requirements.txt</code>\n",
    "<br/>Luego reinicie el kernel (menú Kernel >> Reiniciar kernel y borrar resultados de todas las celdas) para incorporar los cambios.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#f71;\">Recordatorio sobre la página de recursos</h2>\n",
    "<span style=\"color:#f71;\">A continuación, se incluye un enlace a los recursos del curso. Esto incluye enlaces a todas las diapositivas.<br/>\n",
    "<a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "Por favor, mantén este artículo en tus favoritos y seguiré agregando más enlaces útiles allí con el tiempo.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Configuración de las claves\n",
    "\n",
    "Si aún no lo ha hecho, ahora puede crear claves API para Anthropic y Google además de OpenAI.\n",
    "\n",
    "**Nota:** si prefiere evitar costos adicionales de API, ¡no dude en omitir la configuración de Anthopic y Google! Puede verme hacerlo y concentrarse en OpenAI para el curso. También puede sustituir Anthropic o Google por Ollama, utilizando el ejercicio que realizó en la semana 1.\n",
    "\n",
    "Para OpenAI, visite https://openai.com/api/\n",
    "Para Anthropic, visite https://console.anthropic.com/\n",
    "Para Google, visite https://ai.google.dev/gemini-api\n",
    "\n",
    "Cuando obtenga sus claves API, debe configurarlas como variables de entorno agregándolas a su archivo `.env`.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Luego, es posible que tengas que reiniciar el kernel de Jupyter Lab (el proceso de Python que se encuentra detrás de este cuaderno) a través del menú Kernel y, luego, volver a ejecutar las celdas desde la parte superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación para Google\n",
    "# En casos excepcionales, esto parece generar un error en algunos sistemas. Comuníquese conmigo si esto sucede.\n",
    "# O puede omitir Gemini: es la prioridad más baja de los modelos de Frontier que usamos.\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno en un archivo llamado .env\n",
    "# Imprimir los prefijos de clave para facilitar la depuración\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key Sin Configurar\")\n",
    "  \n",
    "if google_api_key:\n",
    "    print(f\"Google API Key existe y empieza por {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key Sin Configurar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conéctate a OpenAI, Anthropic y Google\n",
    "# Las 3 API son similares\n",
    "# ¿Tienes problemas con los archivos API? Puedes usar openai = OpenAI(api_key=\"your-key-here\") y lo mismo para Claude\n",
    "# ¿Tienes problemas con la configuración de Google Gemini? Entonces, omite Gemini; obtendrás toda la experiencia que necesitas de GPT y Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Pedir a los LLM que cuenten un chiste\n",
    "\n",
    "¡Resulta que los LLM no son muy buenos para contar chistes! Comparemos algunos modelos.\n",
    "\n",
    "Más adelante, les daremos un mejor uso a los LLM.\n",
    "\n",
    "### Qué información se incluye en la API\n",
    "\n",
    "Normalmente, pasaremos a la API:\n",
    "- El nombre del modelo que se debe utilizar\n",
    "- Un mensaje del sistema que brinda un contexto general para el rol que desempeña el LLM\n",
    "- Un mensaje del usuario que brinda el mensaje real\n",
    "\n",
    "Hay otros parámetros que se pueden usar, incluida la **temperatura**, que generalmente está entre 0 y 1; más alta para una salida más aleatoria; más baja para una salida más enfocada y determinista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Eres un asistente que es genial contando chistes.\"\n",
    "user_prompt = \"Cuente un chiste divertido para una audiencia de científicos de datos.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "# El ajuste de la temperatura controla la creatividad\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# La API necesita que el mensaje del sistema se proporcione por separado del mensaje del usuario\n",
    "# También se agregaron max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet otra vez\n",
    "# Ahora agreguemos los resultados en streaming\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La API de Gemini tiene una estructura ligeramente diferente\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.5-pro',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡En serio! GPT-4o con la pregunta original\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un asistente útil que responde en Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"¿Cómo puedo decidir si un problema empresarial es adecuado para una solución LLM? Responde en Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos que transmita los resultados en formato Markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## Y ahora, un poco de diversión: una conversación adversaria entre Chatbots...\n",
    "\n",
    "Ya estás familiarizado con las indicaciones organizadas en listas como:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Prompt de Sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Prompt de Usuario\"}\n",
    "]\n",
    "```\n",
    "\n",
    "De hecho, esta estructura se puede utilizar para reflejar un historial de conversación más largo:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Mensaje de sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Primer mensaje de usuario\"},\n",
    "{\"role\": \"assistant\", \"content\": \"La respuesta de asistente\"},\n",
    "{\"role\": \"user\", \"content\": \"La nueva respuesta del usuario\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Y podemos utilizar este enfoque para participar en una interacción más larga con el historial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos una conversación entre GPT-4o-mini y Claude-3-haiku\n",
    "# Estamos usando versiones económicas de los modelos, por lo que los costos serán mínimos\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"Eres un chatbot muy argumentativo; \\\n",
    "no estás de acuerdo con nada en la conversación y cuestionas todo de manera sarcástica.\"\n",
    "\n",
    "claude_system = \"Eres un chatbot muy educado y cortés. Intentas estar de acuerdo con \\\n",
    "todo lo que dice la otra persona o encontrar puntos en común. Si la otra persona discute, \\\n",
    "intentas calmarla y seguir charlando.\"\n",
    "\n",
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Antes de continuar</h2>\n",
    "<span style=\"color:#900;\">\n",
    "Asegúrese de comprender cómo funciona la conversación anterior y, en particular, cómo se completa la lista de <code>mensajes</code>. Agregue declaraciones de impresión según sea necesario. Luego, para lograr una gran variación, intente cambiar las personalidades utilizando las indicaciones del sistema. ¿Quizás una pueda ser pesimista y la otra optimista?<br/>\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# Ejercicios más avanzados\n",
    "\n",
    "¡Intenta crear un modelo de 3 vías, quizás incorporando a Gemini a la conversación! Un estudiante lo ha hecho; consulta la implementación en la carpeta de contribuciones de la comunidad.\n",
    "\n",
    "Intenta hacerlo tú mismo antes de ver las soluciones.\n",
    "\n",
    "## Ejercicio adicional\n",
    "\n",
    "También puedes intentar reemplazar uno de los modelos con un modelo de código abierto que se ejecute con Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#181;\">Relevancia para el negocio</h2>\n",
    "<span style=\"color:#181;\">Esta estructura de una conversación, como una lista de mensajes, es fundamental para la forma en que construimos asistentes de IA conversacionales y cómo pueden mantener el contexto durante una conversación. Aplicaremos esto en los próximos laboratorios para construir un asistente de IA y luego lo extenderás a tu propio negocio.</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
